{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path to the data\n",
    "pathCOMPASOutput = '/home/cneijssel/Documents/Projects/Data/CosmicInt/'\n",
    "#path to compasrepo which contains ther popsynth folder\n",
    "pathRepo = '/home/cneijssel/Documents/COMPASpop'\n",
    "#path to Bootstrap\n",
    "pathBootstrap = '/home/cneijssel/Documents/Projects/Data/CosmicInt/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "#Quick fudge to make import from ../Scripts work\n",
    "import sys\n",
    "sys.path.append(pathRepo + '/popsynth/Papers/NeijsselEtAL/CosmicIntegration/Scripts')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Imports from COMPASrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ClassCosmicIntegrator  as CI #Given settings and redshifts returns rates (2D arrays) Loads the data\n",
    "import ClassBootStrap         as CB #pre sampled from initial seeds per metallicity the reader is there too\n",
    "import coencodeVarious        as CV\n",
    "import ClassBayes             as CL  #Used to create KDECOMPAS/Posterior/Likelyhood \n",
    "import ClassEvents            as CE  #Where I store the GW-events observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook we cerated an h5 file with the rate per chirp mass per system per variation. In this notebook we will calculate the likelyhood per event and the total likelyhood. Note that for each individual likely hood, whether it is the sum or not, we need to bootstrap individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5file           = h5.File('VariationIntegration.h5')\n",
    "\n",
    "# Results        a 2D array of 18 variations 2X3X3 (rows) and \n",
    "#                              5 types of rates 2X3 (columns) and + - error so 15 columns\n",
    "results          = np.zeros(shape=(18,15))\n",
    "\n",
    "#keys are not given sorted, however in the sorted order it is easier to make the table\n",
    "#convert list of string numbers to integers, get the arguments to sort and use these\n",
    "#to sort keys\n",
    "a = np.array(h5file.keys())\n",
    "b = np.argsort(a.astype(int))\n",
    "\n",
    "#for v in a[b]:\n",
    "    #print h5file[v]['SFR_MZ_GSMF'][...]\n",
    "#    print h5file[v]['BBH'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap and Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have premade a bootsStrap wit ClassBootstrap, this is a resampling off all seeds per metallicity with replacement. From that I count of *all* the DCOs in my double compact object file how often they appear. \n",
    "That is stored as a column to file. Hence if I have a maskDCO of one file say cosmic integration, that same maskDCO applied to the bootstrap file gives me the nr of draws. Hence here I only have to recover the mask, and apply to bootstrap file to get counts. Then I sort the counts and get +- 90% uncertainty\n",
    "\n",
    "\n",
    "ClassEvents contains the information (as of yet) fo the events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GW150914' 'GW151012' 'GW151226' 'GW170104' 'GW170608' 'GW170729'\n",
      " 'GW170809' 'GW170814' 'GW170818' 'GW170823'] ['O1' 'O1' 'O1' 'O2' 'O2' 'O2' 'O2' 'O2' 'O2' 'O2']\n"
     ]
    }
   ],
   "source": [
    "boot   = CB.Bootstrap(pathToFile = pathBootstrap, fileName='Bootstrap5.h5')\n",
    "\n",
    "def bootstrap(Likelihoods, cene):\n",
    "    #order them from low to high and give error \n",
    "    Rates     = np.sort(Likelihoods) - np.sum(ratePerSystem)\n",
    "    #If you sort array at what index do you find lower and upper confidence 90%\n",
    "    lower     = int((nrSamples/100.)*5)\n",
    "    upper     = int((nrSamples/100.)*95)\n",
    "    return Rates[lower], Rates[upper]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "events = CE.Observations()    \n",
    "\n",
    "names, values, lowerErrors, upperErrors, surveys =\\\n",
    "events.giveParameterAndError(parameter='Mchirp', types=['BBH'])\n",
    "print names, surveys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Likelihoods(chirpMasses, rates, maskDCO):\n",
    "    \n",
    "\n",
    "    nrSamples = 200\n",
    "\n",
    "    \n",
    "    #calculate the width of the kernel using scipy in Bayes class\n",
    "    bayes = CL.BayesianLikelyhoodSingleObservation(datapoints=chirpMasses)\n",
    "    \n",
    "    \n",
    "    ###############################################\n",
    "    #                                             #\n",
    "    #           Likelihoods chirp masses events   #\n",
    "    #                                             #\n",
    "    ###############################################\n",
    "    \n",
    "    centerValuesLMc = np.zeros(len(names))\n",
    "\n",
    "    #I want to do bootstrapping but the summing the error bars from\n",
    "    #individual bootstrapping is not the same as the error bars from\n",
    "    #bootstrapping over all events together.\n",
    "    #dont want to keep recalculating the probabilities twice per event\n",
    "    # Hence I store in array and sum correctly in post processing.\n",
    "    #per event I need to store X numbers, where X is the number of\n",
    "    #bootstrappings.\n",
    "    LikelihoodsEvents = np.zeros(shape=(len(names),nrSamples))\n",
    "    for nrB, observation in enumerate(names):\n",
    "        #prepare the gaussians we are going to use for the observation\n",
    "        #and the dx steps for integrating\n",
    "        bayes.setObservation(observation=values[nrB], lowerError=lowerErrors[nrB], \\\n",
    "                             upperError=upperErrors[nrB], confidence=0.9)\n",
    "        \n",
    "        #What is our center value (i.e. with original rate)\n",
    "        bayes.calculatePDFs(weights=rates)\n",
    "        centerValuesLMc[nrB] = np.log10(bayes._PmodelGivenObservation )    \n",
    "        #Now we have pre-bootstrapped the rates, so redo\n",
    "        #last two lines per bootstrap and store in array\n",
    "        for sample in range(nrSamples):\n",
    "            #Reset rates according to Bootstrap sample\n",
    "            s = boot.ReadSample(nrSample=sample, maskDCO=maskDCO)\n",
    "            rateSample = np.multiply(s, rates)\n",
    "            #recalculate the PDFs and store likelyhood\n",
    "            bayes.calculatePDFs2(weights=rateSample)\n",
    "            LikelihoodsEvents[nrB][sample] = np.log10(bayes._PmodelGivenObservation)\n",
    "    #######################################################\n",
    "    #        Calculating  Poisson statistic               #\n",
    "    #######################################################\n",
    "    \n",
    "    surveyDurationsYr = {'O1':48./365.25, 'O2':118./365.25}\n",
    "    Surveys            = np.unique(surveys)\n",
    "    centerValuesLR    = np.zeros(len(Surveys))\n",
    "    LikelihoodsRates  = np.zeros(shape=(len(Surveys),nrSamples))\n",
    "    \n",
    "    for nrs, survey in enumerate(np.unique(surveys)):\n",
    "        #which events are in survey\n",
    "        mask          = (surveys == survey)\n",
    "        durationYr    = surveyDurationsYr[survey]\n",
    "        nrObservations= np.sum(mask)\n",
    "        \n",
    "        rateCOMPASduringSurvey = np.sum(rates) * durationYr\n",
    "        centerValuesLR[nrs]    = np.log10(bayes.poisson(rateCOMPASduringSurvey, nrObservations))\n",
    "        for sample in range(nrSamples):\n",
    "            #Reset rates according to Bootstrap sample\n",
    "            s                        = boot.ReadSample(nrSample=sample, maskDCO=maskDCO)\n",
    "            rateSample               = np.multiply(s, rates)\n",
    "            rateCOMPASduringSurvey   = np.sum(rateSample) * durationYr\n",
    "            LikelihoodsRates[nrs][sample] = np.log10(bayes.poisson(rateCOMPASduringSurvey, nrObservations))        \n",
    "    \n",
    "    ########################################################\n",
    "    #      Combining likleyhoods                           #\n",
    "    ########################################################\n",
    "    #there must be a prettier way but no time nor brain for it now apologies\n",
    "    \n",
    "    #what I want is  McO1-+ RO1-+ McO2-+ RO2-+ Ltot-+ 5*3 entries\n",
    "    results = np.zeros(15)\n",
    "    \n",
    "    #If you sort array at what index do you find lower and upper confidence 90%\n",
    "    lower     = int((nrSamples/100.)*5)\n",
    "    upper     = int((nrSamples/100.)*95)\n",
    "    \n",
    "    #reduce, sum,  and sort - center values is error\n",
    "    maskO1      = (surveys == 'O1')\n",
    "    centerMcO1  = np.sum(centerValuesLMc[maskO1])\n",
    "    errorMcO1   = np.sort(np.sum(LikelihoodsEvents[maskO1], axis=0))- centerMcO1\n",
    "    \n",
    "    \n",
    "    centerRO1   = centerValuesLR[0]\n",
    "    errorRO1    = np.sort(LikelihoodsRates[0])- centerRO1\n",
    "    \n",
    "    results[0]  = centerMcO1\n",
    "    results[1]  = errorMcO1[lower]\n",
    "    results[2]  = errorMcO1[upper]\n",
    "    results[3]  = centerRO1\n",
    "    results[4]  = errorRO1[lower]\n",
    "    results[5]  = errorRO1[upper]\n",
    " \n",
    "    #reduce, sum,  and sort - center values is error\n",
    "    maskO2      = (surveys == 'O2')\n",
    "    centerMcO2  = np.sum(centerValuesLMc[maskO2])\n",
    "    errorMcO2   = np.sort(np.sum(LikelihoodsEvents[maskO2], axis=0))- centerMcO2\n",
    "    centerRO2   = centerValuesLR[1]\n",
    "    errorRO2    = np.sort(LikelihoodsRates[1])- centerRO2\n",
    "    \n",
    "    results[6]  = centerMcO2\n",
    "    results[7]  = errorMcO2[lower]\n",
    "    results[8]  = errorMcO2[upper]\n",
    "    results[9]  = centerRO2\n",
    "    results[10] = errorRO2[lower]\n",
    "    results[11] = errorRO2[upper]\n",
    "    \n",
    "    #total is sum of all likelhoods\n",
    "    centerValue = np.sum(centerValuesLMc) + np.sum(centerValuesLR)\n",
    "    LtotMc      = np.sum(LikelihoodsEvents, axis=0)\n",
    "    LtotR       = np.sum(LikelihoodsRates, axis=0)\n",
    "    LtotError   = np.sort(LtotMc+LtotR) - centerValue\n",
    "\n",
    "    results[12] = centerValue\n",
    "    results[13] = LtotError[lower]\n",
    "    results[14] = LtotError[upper]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "redoCalculation = False\n",
    "\n",
    "if redoCalculation:\n",
    "    #18 variations 15 entries\n",
    "    results          = np.zeros(shape=(18,15))\n",
    "    for Pessimistic in [False, True]:\n",
    "        counterRow = 0\n",
    "        for nrv, v in enumerate(a[b]):\n",
    "            print h5file[v]['SFR_MZ_GSMF'][...]\n",
    "            counterColumn   = 0\n",
    "                 \n",
    "            if Pessimistic:\n",
    "                maskPessimistic = h5file[v]['BBH']['Pessimistic'][...].squeeze() == True\n",
    "            else:\n",
    "                #want them all\n",
    "                maskPessimistic = np.ones(len(h5file[v]['BBH']['Pessimistic'][...].squeeze()), dtype=bool)\n",
    "            chirpMasses   =  h5file[v]['BBH']['chirpMass'][...].squeeze()[maskPessimistic]\n",
    "            rates         =  h5file[v]['BBH']['ObservedRate'][...].squeeze()[maskPessimistic]\n",
    "            seedsInterest =  h5file[v]['BBH']['randomSeed'][...].squeeze()[maskPessimistic]\n",
    "            \n",
    "            #recover maskDCO for Bootstrap   \n",
    "            originalDCOfile = h5.File(pathCOMPASOutput+'COMPASOutput.h5')\n",
    "            seedsOriginal =  originalDCOfile['doubleCompactObjects']['seed'][...].squeeze()\n",
    "            maskDCO       =  np.in1d(seedsOriginal, seedsInterest)\n",
    "\n",
    "            results[nrv]   =  Likelihoods(chirpMasses, rates, maskDCO)\n",
    "            print results[nrv]\n",
    "        if Pessimistic:\n",
    "            np.savetxt('LikelihoodsPessimistic.txt', results)\n",
    "        else:\n",
    "            np.savetxt('LikelihoodsOptimistic.txt', results)\n",
    "        print 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}\n",
      "\\begin{tabular}{|l |l | l| lllll}\n",
      "\\multicolumn{3}{|c|}{Variation MSSFR} & \\multicolumn{5}{|c|}{ Likelihoods}  \\\\\n",
      "SFR & GSMF & MZ & $\\rm \\mathcal{L}_{Mc, O1}$ &   $\\rm \\mathcal{L}_{R, O1}$  &$\\rm \\mathcal{L}_{Mc, O2}$ &   $\\rm \\mathcal{L}_{R, O2}$ & $\\rm \\mathcal{L}_{tot}$ \\\\ \\hline \\hline\n",
      " &&&&&&&\\\\\n",
      "\\multicolumn{8}{|c|}{Pessimistic} \\\\\n",
      " &&&&&&&\\\\\n",
      "Madau et al. & Ma et al. (2004) & 1 &$ -9.72_{-0.08}^{+0.08} $ & $ -0.97_{-0.02}^{+0.01} $ & $ -24.18_{-0.13}^{+0.12} $ & $ 0.0_{0.0}^{+0.0} $ & $ -34.87_{-0.16}^{+0.14}$  \\\\\n",
      " & & 2 &$ -9.87_{-0.05}^{+0.05} $ & $ -8.86_{-0.21}^{+0.21} $ & $ -22.55_{-0.07}^{+0.05} $ & $ 0.0_{0.0}^{+0.0} $ & $ -41.28_{-0.26}^{+0.24}$  \\\\\n",
      " & & 3 &$ -9.94_{-0.05}^{+0.04} $ & $ -11.9_{-0.18}^{+0.19} $ & $ -22.54_{-0.06}^{+0.05} $ & $ 0.0_{0.0}^{+0.0} $ & $ -44.38_{-0.22}^{+0.23}$  \\\\\\cline{2-8}\n",
      " & Langer et al.  &1 &$ -9.77_{-0.03}^{+0.03} $ & $ -34.85_{-0.47}^{+0.5} $ & $ -22.47_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -67.09_{-0.47}^{+0.49}$  \\\\\n",
      " &  &2 &$ -10.06_{-0.03}^{+0.02} $ & $ -70.6_{-1.05}^{+1.23} $ & $ -22.54_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -103.21_{-1.05}^{+1.22}$  \\\\\n",
      " &  &3 &$ -10.14_{-0.03}^{+0.03} $ & $ -80.23_{-1.04}^{+1.32} $ & $ -22.63_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -113.0_{-1.09}^{+1.3}$  \\\\\\cline{2-8}\n",
      " & Langer et al., offset  &1 &$ -9.75_{-0.06}^{+0.05} $ & $ -1.07_{-0.02}^{+0.02} $ & $ -22.56_{-0.08}^{+0.08} $ & $ 0.0_{0.0}^{+0.0} $ & $ -33.38_{-0.1}^{+0.09}$  \\\\\n",
      " &  &2 &$ -10.12_{-0.04}^{+0.04} $ & $ -12.93_{-0.33}^{+0.38} $ & $ -22.56_{-0.07}^{+0.06} $ & $ 0.0_{0.0}^{+0.0} $ & $ -45.61_{-0.38}^{+0.42}$  \\\\\n",
      " &  &3 &$ -10.23_{-0.04}^{+0.04} $ & $ -17.62_{-0.33}^{+0.35} $ & $ -22.64_{-0.06}^{+0.05} $ & $ 0.0_{0.0}^{+0.0} $ & $ -50.49_{-0.36}^{+0.38}$  \\\\\\hline\n",
      "Strolger et al. & Ma et al. (2004) & 1 &$ -9.8_{-0.08}^{+0.08} $ & $ -1.31_{-0.05}^{+0.06} $ & $ -24.03_{-0.13}^{+0.12} $ & $ 0.0_{0.0}^{+0.0} $ & $ -35.13_{-0.17}^{+0.18}$  \\\\\n",
      " & &2 &$ -10.17_{-0.06}^{+0.06} $ & $ -27.14_{-0.91}^{+0.87} $ & $ -22.64_{-0.08}^{+0.08} $ & $ 0.0_{0.0}^{+0.0} $ & $ -59.95_{-0.97}^{+0.93}$  \\\\\n",
      " & &3 &$ -10.11_{-0.04}^{+0.05} $ & $ -27.9_{-0.58}^{+0.61} $ & $ -22.54_{-0.06}^{+0.05} $ & $ 0.0_{0.0}^{+0.0} $ & $ -60.54_{-0.61}^{+0.63}$  \\\\\\cline{2-8}\n",
      " & Langer et al. &1 &$ -10.0_{-0.04}^{+0.03} $ & $ -64.11_{-0.97}^{+1.08} $ & $ -22.44_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -96.55_{-1.02}^{+1.08}$  \\\\\n",
      " &  &2 &$ -10.31_{-0.05}^{+0.04} $ & $ -111.92_{-2.81}^{+2.83} $ & $ -22.68_{-0.07}^{+0.07} $ & $ 0.0_{0.0}^{+0.0} $ & $ -144.9_{-2.83}^{+2.9}$  \\\\\n",
      " &  &3 &$ -10.36_{-0.05}^{+0.04} $ & $ -121.79_{-2.83}^{+2.8} $ & $ -22.73_{-0.07}^{+0.07} $ & $ 0.0_{0.0}^{+0.0} $ & $ -154.87_{-2.9}^{+2.86}$  \\\\\\cline{2-8}\n",
      " & Langer et al., offset &1 &$ -10.03_{-0.06}^{+0.06} $ & $ -8.18_{-0.21}^{+0.21} $ & $ -22.42_{-0.08}^{+0.07} $ & $ 0.0_{0.0}^{+0.0} $ & $ -40.63_{-0.26}^{+0.23}$  \\\\\n",
      " &  &2 &$ -10.44_{-0.07}^{+0.06} $ & $ -38.48_{-1.48}^{+1.44} $ & $ -22.76_{-0.09}^{+0.08} $ & $ 0.0_{0.0}^{+0.0} $ & $ -71.68_{-1.51}^{+1.51}$  \\\\\n",
      " &  &3 &$ -10.46_{-0.06}^{+0.06} $ & $ -43.1_{-1.27}^{+1.29} $ & $ -22.76_{-0.08}^{+0.08} $ & $ 0.0_{0.0}^{+0.0} $ & $ -76.33_{-1.34}^{+1.35}$  \\\\\n",
      "\\hline \n",
      " &&&&&&&\\\\\n",
      "\\multicolumn{8}{|c|}{Optimistic} \\\\\n",
      " &&&&&&&\\\\\n",
      "Madau et al. & Ma et al. (2004) & 1 &$ -10.03_{-0.04}^{+0.05} $ & $ -2.46_{-0.07}^{+0.07} $ & $ -26.34_{-0.1}^{+0.09} $ & $ 0.0_{0.0}^{+0.0} $ & $ -38.84_{-0.14}^{+0.16}$  \\\\\n",
      " & & 2 &$ -9.59_{-0.03}^{+0.03} $ & $ -14.24_{-0.23}^{+0.25} $ & $ -23.02_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -46.85_{-0.26}^{+0.27}$  \\\\\n",
      " & & 3 &$ -9.62_{-0.03}^{+0.02} $ & $ -17.59_{-0.21}^{+0.24} $ & $ -22.9_{-0.04}^{+0.03} $ & $ 0.0_{0.0}^{+0.0} $ & $ -50.1_{-0.22}^{+0.24}$  \\\\\\cline{2-8}\n",
      " & Langer et al.  &1 &$ -9.52_{-0.02}^{+0.03} $ & $ -50.38_{-0.52}^{+0.52} $ & $ -23.03_{-0.03}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -82.93_{-0.53}^{+0.5}$  \\\\\n",
      " &  &2 &$ -9.77_{-0.02}^{+0.02} $ & $ -85.01_{-0.98}^{+1.21} $ & $ -22.78_{-0.04}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -117.56_{-1.02}^{+1.23}$  \\\\\n",
      " &  &3 &$ -9.83_{-0.02}^{+0.02} $ & $ -95.41_{-1.07}^{+1.28} $ & $ -22.84_{-0.04}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -128.08_{-1.11}^{+1.29}$  \\\\\\cline{2-8}\n",
      " & Langer et al., offset  &1 &$ -9.55_{-0.03}^{+0.04} $ & $ -2.18_{-0.05}^{+0.05} $ & $ -23.3_{-0.06}^{+0.06} $ & $ 0.0_{0.0}^{+0.0} $ & $ -35.03_{-0.08}^{+0.09}$  \\\\\n",
      " &  &2 &$ -9.77_{-0.03}^{+0.03} $ & $ -16.48_{-0.37}^{+0.39} $ & $ -22.72_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -48.97_{-0.39}^{+0.43}$  \\\\\n",
      " &  &3 &$ -9.86_{-0.03}^{+0.03} $ & $ -21.27_{-0.36}^{+0.35} $ & $ -22.72_{-0.04}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -53.85_{-0.36}^{+0.38}$  \\\\\\hline\n",
      "Strolger et al. & Ma et al. (2004) & 1 &$ -9.88_{-0.04}^{+0.05} $ & $ -4.58_{-0.14}^{+0.15} $ & $ -25.48_{-0.11}^{+0.09} $ & $ 0.0_{0.0}^{+0.0} $ & $ -39.95_{-0.19}^{+0.21}$  \\\\\n",
      " & &2 &$ -9.8_{-0.04}^{+0.04} $ & $ -33.45_{-0.99}^{+0.89} $ & $ -22.74_{-0.06}^{+0.06} $ & $ 0.0_{0.0}^{+0.0} $ & $ -65.99_{-0.98}^{+0.95}$  \\\\\n",
      " & &3 &$ -9.75_{-0.03}^{+0.03} $ & $ -34.37_{-0.6}^{+0.62} $ & $ -22.65_{-0.05}^{+0.04} $ & $ 0.0_{0.0}^{+0.0} $ & $ -66.77_{-0.62}^{+0.65}$  \\\\\\cline{2-8}\n",
      " & Langer et al. &1 &$ -9.68_{-0.03}^{+0.03} $ & $ -79.19_{-1.08}^{+1.11} $ & $ -22.7_{-0.04}^{+0.03} $ & $ 0.0_{0.0}^{+0.0} $ & $ -111.57_{-1.11}^{+1.14}$  \\\\\n",
      " &  &2 &$ -9.99_{-0.04}^{+0.03} $ & $ -125.54_{-2.83}^{+2.79} $ & $ -22.78_{-0.06}^{+0.06} $ & $ 0.0_{0.0}^{+0.0} $ & $ -158.32_{-2.86}^{+2.82}$  \\\\\n",
      " &  &3 &$ -10.03_{-0.04}^{+0.03} $ & $ -136.06_{-2.82}^{+2.78} $ & $ -22.83_{-0.06}^{+0.05} $ & $ 0.0_{0.0}^{+0.0} $ & $ -168.92_{-2.88}^{+2.79}$  \\\\\\cline{2-8}\n",
      " & Langer et al., offset &1 &$ -9.67_{-0.04}^{+0.04} $ & $ -10.91_{-0.21}^{+0.24} $ & $ -22.57_{-0.06}^{+0.05} $ & $ 0.0_{0.0}^{+0.0} $ & $ -43.15_{-0.24}^{+0.23}$  \\\\\n",
      " &  &2 &$ -10.09_{-0.05}^{+0.05} $ & $ -42.69_{-1.51}^{+1.46} $ & $ -22.74_{-0.08}^{+0.07} $ & $ 0.0_{0.0}^{+0.0} $ & $ -75.52_{-1.51}^{+1.53}$  \\\\\n",
      " &  &3 &$ -10.11_{-0.04}^{+0.04} $ & $ -47.34_{-1.35}^{+1.31} $ & $ -22.73_{-0.07}^{+0.06} $ & $ 0.0_{0.0}^{+0.0} $ & $ -80.18_{-1.36}^{+1.31}$  \\\\\n",
      "\\end{tabular}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "o  = np.loadtxt('LikelihoodsOptimistic.txt')\n",
    "p = np.loadtxt('LikelihoodsPessimistic.txt')\n",
    "\n",
    "starter     = ['Madau et al. & Ma et al. (2004) & 1 &',' & & 2 &',' & & 3 &',\n",
    "               ' & Langer et al.  &1 &',\\\n",
    "               ' &  &2 &',\\\n",
    "               ' &  &3 &',\\\n",
    "               ' & Langer et al., offset  &1 &',\\\n",
    "               ' &  &2 &',\\\n",
    "               ' &  &3 &',\\\n",
    "               'Strolger et al. & Ma et al. (2004) & 1 &',\\\n",
    "               ' & &2 &',\\\n",
    "               ' & &3 &',\\\n",
    "               ' & Langer et al. &1 &',\\\n",
    "               ' &  &2 &',\\\n",
    "               ' &  &3 &',\\\n",
    "               ' & Langer et al., offset &1 &',\\\n",
    "               ' &  &2 &',\\\n",
    "               ' &  &3 &',\\\n",
    "              ]\n",
    "print r'\\begin{table*}'\n",
    "print r'\\begin{tabular}{|l |l | l| lllll}'\n",
    "print r'\\multicolumn{3}{|c|}{Variation MSSFR} & \\multicolumn{5}{|c|}{ Likelihoods}  \\\\'\n",
    "print r'SFR & GSMF & MZ & $\\rm \\mathcal{L}_{Mc, O1}$ &   $\\rm \\mathcal{L}_{R, O1}$  &$\\rm \\mathcal{L}_{Mc, O2}$ &   $\\rm \\mathcal{L}_{R, O2}$ & $\\rm \\mathcal{L}_{tot}$ \\\\ \\hline \\hline'\n",
    "print r' &&&&&&&\\\\'\n",
    "print r'\\multicolumn{8}{|c|}{Pessimistic} \\\\'\n",
    "print r' &&&&&&&\\\\'\n",
    "\n",
    "\n",
    "LMc   = \n",
    "\n",
    "for nrV in range(len(h5file.keys())):\n",
    "    string = starter[nrV]\n",
    "    row    = p[nrV]\n",
    "    row = np.round(row, 2)\n",
    "    string+= r'$ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s}$  \\\\'\\\n",
    "    %(row[0],row[1], row[2],row[3],row[4],row[5],row[6],row[7],row[8],\\\n",
    "      row[9],row[10],row[11],row[12],row[13],row[14])\n",
    "        \n",
    "    if nrV in [2,5,11,14]:\n",
    "        string += '\\cline{2-8}'\n",
    "    if nrV == 8:\n",
    "        string += '\\hline'\n",
    "    print string\n",
    "\n",
    "print r'\\hline '\n",
    "print r' &&&&&&&\\\\'\n",
    "print r'\\multicolumn{8}{|c|}{Optimistic} \\\\'    \n",
    "print r' &&&&&&&\\\\'\n",
    "\n",
    "for nrV in range(len(h5file.keys())):\n",
    "    string = starter[nrV]\n",
    "    row    = o[nrV]\n",
    "    row = np.round(row, 2)\n",
    "    string+= r'$ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s} $ & $ %s_{%s}^{+%s}$  \\\\'\\\n",
    "    %(row[0],row[1], row[2],row[3],row[4],row[5],row[6],row[7],row[8],\\\n",
    "      row[9],row[10],row[11],row[12],row[13],row[14])\n",
    "        \n",
    "    if nrV in [2,5,11,14]:\n",
    "        string += '\\cline{2-8}'\n",
    "    if nrV == 8:\n",
    "        string += '\\hline'\n",
    "    print string\n",
    "\n",
    "print r'\\end{tabular}'\n",
    "print r'\\end{table*}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
